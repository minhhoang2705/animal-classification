{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "from PIL.Image import Image\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import patoolib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_rar = 'D:/animals classification/Dataset.rar'\n",
    "\n",
    "patoolib.extract_archive('Dataset.rar', outdir=\"/animals classification/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1668 images of cats\n",
      "There are 4863 images of dogs\n",
      "There are 3098 images of chickens\n",
      "There are 1862 images of squirrels\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "source_path = 'D:/animals classification/Dataset/'\n",
    "\n",
    "source_path_cats = os.path.join(source_path, 'cats')\n",
    "source_path_chickens = os.path.join(source_path, 'chicken')\n",
    "source_path_dogs = os.path.join(source_path, 'dogs')\n",
    "source_path_squirrels = os.path.join(source_path, 'squirrels')\n",
    "\n",
    "print(f\"There are {len(os.listdir(source_path_cats))} images of cats\")\n",
    "print(f\"There are {len(os.listdir(source_path_dogs))} images of dogs\")\n",
    "print(f\"There are {len(os.listdir(source_path_chickens))} images of chickens\")\n",
    "print(f\"There are {len(os.listdir(source_path_squirrels))} images of squirrels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "root_dir = \"D:/animals classification/animals\"\n",
    "\n",
    "if os.path.exists(root_dir):\n",
    "    shutil.rmtree(root_dir)\n",
    "\n",
    "\n",
    "def create_train_val_dirs(root_path):\n",
    "    os.makedirs(root_path)\n",
    "    os.makedirs(os.path.join(root_path, \"training\"))\n",
    "    os.makedirs(os.path.join(root_path, \"training\", \"cats\"))\n",
    "    os.makedirs(os.path.join(root_path, \"training\", \"dogss\"))\n",
    "    os.makedirs(os.path.join(root_path, \"training\", \"chickens\"))\n",
    "    os.makedirs(os.path.join(root_path, \"training\", \"squirrels\"))\n",
    "    os.makedirs(os.path.join(root_path, \"validation\"))\n",
    "    os.makedirs(os.path.join(root_path, \"validation\", \"cats\"))\n",
    "    os.makedirs(os.path.join(root_path, \"validation\", \"dogs\"))\n",
    "    os.makedirs(os.path.join(root_path, \"validation\", \"chickens\"))\n",
    "    os.makedirs(os.path.join(root_path, \"validation\", \"squirrels\"))\n",
    "    pass\n",
    "\n",
    "\n",
    "try:\n",
    "    create_train_val_dirs(root_path=root_dir)\n",
    "except:\n",
    "    print(\n",
    "        \"You should not be seeing this since the upper directory is removed beforehand\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rootdir, dirs, files in os.walk(root_dir):\n",
    "    for subdir in dirs:\n",
    "        print(os.path.join(rootdir, subdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "\n",
    "def split_data(SOURCE_DIR, TRAINING_DIR, VALIDATION_DIR, SPLIT_SIZE):\n",
    "    filtered_files = []\n",
    "    for filename in os.listdir(SOURCE_DIR):\n",
    "        file_path = os.path.join(SOURCE_DIR, filename)\n",
    "        if os.path.getsize(file_path):\n",
    "            filtered_files.append(filename)\n",
    "        else:\n",
    "            print(f\"{filename} is zero length, so ignoring\")\n",
    "    random_files = random.sample(filtered_files, len(filtered_files))\n",
    "    split = int(SPLIT_SIZE * len(filtered_files))\n",
    "    training_files = random_files[:split]\n",
    "    validation_files = random_files[split:]\n",
    "\n",
    "    for filename in training_files:\n",
    "        copyfile(\n",
    "            os.path.join(SOURCE_DIR, filename), os.path.join(TRAINING_DIR, filename)\n",
    "        )\n",
    "    for filename in validation_files:\n",
    "        copyfile(\n",
    "            os.path.join(SOURCE_DIR, filename), os.path.join(VALIDATION_DIR, filename)\n",
    "        )\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATS_SOURCE_DIR = 'D:/animals classification/Dataset/cats'\n",
    "DOGS_SOURCE_DIR = 'D:/animals classification/Dataset/dogs'\n",
    "CHICKENS_SOURCE_DIR = 'D:/animals classification/Dataset/chicken'\n",
    "SQUIRRELS_SOURCE_DIR = 'D:/animals classification/Dataset/squirrels'\n",
    "\n",
    "TRAINING_DIR = 'D:/animals classification/animals/training/'\n",
    "VALIDATION_DIR = 'D:/animals classification/animals/validation/'\n",
    "\n",
    "TRAINING_CATS_DIR = os.path.join(TRAINING_DIR, 'cats')\n",
    "VALIDATION_CATS_DIR = os.path.join(VALIDATION_DIR, 'cats')\n",
    "\n",
    "TRAINING_DOGS_DIR = os.path.join(TRAINING_DIR, 'dogss')\n",
    "VALIDATION_DOGS_DIR = os.path.join(VALIDATION_DIR, 'dogs')\n",
    "\n",
    "TRAINING_CHICKENS_DIR = os.path.join(TRAINING_DIR, 'chickens')\n",
    "VALIDATION_CHICKENS_DIR = os.path.join(VALIDATION_DIR, 'chickens')\n",
    "\n",
    "TRAINING_SQUIRRELS_DIR = os.path.join(TRAINING_DIR, 'squirrels')\n",
    "VALIDATION_SQUIRRELS_DIR = os.path.join(VALIDATION_DIR, 'squirrels')\n",
    "\n",
    "if len(os.listdir(TRAINING_CATS_DIR)) > 0:\n",
    "    for file in os.scandir(TRAINING_CATS_DIR):\n",
    "        os.remove(file.path)\n",
    "if len(os.listdir(TRAINING_DOGS_DIR)) > 0:\n",
    "    for file in os.scandir(TRAINING_DOGS_DIR):\n",
    "        os.remove(file.path)\n",
    "if len(os.listdir(TRAINING_CHICKENS_DIR)) > 0:\n",
    "    for file in os.scandir(TRAINING_CHICKENS_DIR):\n",
    "        os.remove(file.path)\n",
    "if len(os.listdir(TRAINING_SQUIRRELS_DIR)) > 0:\n",
    "    for file in os.scandir(TRAINING_SQUIRRELS_DIR):\n",
    "        os.remove(file.path)\n",
    "\n",
    "split_size = .8\n",
    "\n",
    "split_data(CATS_SOURCE_DIR, TRAINING_CATS_DIR, VALIDATION_CATS_DIR, split_size)\n",
    "split_data(DOGS_SOURCE_DIR, TRAINING_DOGS_DIR, VALIDATION_DOGS_DIR, split_size)\n",
    "split_data(CHICKENS_SOURCE_DIR, TRAINING_CHICKENS_DIR, VALIDATION_CHICKENS_DIR, split_size)\n",
    "split_data(SQUIRRELS_SOURCE_DIR, TRAINING_SQUIRRELS_DIR, VALIDATION_SQUIRRELS_DIR, split_size)\n",
    "\n",
    "print(f\"\\nOriginal chicken's dir has {len(os.listdir(CHICKENS_SOURCE_DIR))} images\")\n",
    "print(f\"Original cats's dir has {len(os.listdir(CATS_SOURCE_DIR))} images\")\n",
    "print(f\"Original dog's dir has {len(os.listdir(DOGS_SOURCE_DIR))} images\")\n",
    "print(f\"Original squirrel's dir has {len(os.listdir(SQUIRRELS_SOURCE_DIR))} images\\n\")\n",
    "\n",
    "print(f\"There are {len(os.listdir(TRAINING_CATS_DIR))} images of cats for training\")\n",
    "print(f\"There are {len(os.listdir(VALIDATION_CATS_DIR))} images of cats for validation\")\n",
    "\n",
    "print(f\"There are {len(os.listdir(TRAINING_DOGS_DIR))} images of dogs for training\")\n",
    "print(f\"There are {len(os.listdir(VALIDATION_DOGS_DIR))} images of dogs for validation\")\n",
    "\n",
    "print(f\"There are {len(os.listdir(TRAINING_CHICKENS_DIR))} images of chickens for training\")\n",
    "print(f\"There are {len(os.listdir(VALIDATION_CHICKENS_DIR))} images of chickens for validation\")\n",
    "\n",
    "print(f\"There are {len(os.listdir(TRAINING_SQUIRRELS_DIR))} images of squirrels for training\")\n",
    "print(f\"There are {len(os.listdir(VALIDATION_SQUIRRELS_DIR))} images of squirrels for validation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "def train_val_generators(TRAINING_DIR, VALIDATION_DIR):\n",
    "    train_data_gen = ImageDataGenerator(\n",
    "        rescale=1.0 / 255,\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode=\"nearest\",\n",
    "    )\n",
    "\n",
    "    train_generator = train_data_gen.flow_from_directory(\n",
    "        directory=TRAINING_DIR,\n",
    "        batch_size=64,\n",
    "        shuffle=True,\n",
    "        class_mode=\"categorical\",\n",
    "        target_size=(150, 150),\n",
    "    )\n",
    "\n",
    "    val_data_gen = ImageDataGenerator(\n",
    "        rescale=1.0 / 255,\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode=\"nearest\",\n",
    "    )\n",
    "\n",
    "    val_generator = val_data_gen.flow_from_directory(\n",
    "        directory=VALIDATION_DIR,\n",
    "        batch_size=64,\n",
    "        shuffle=True,\n",
    "        class_mode=\"categorical\",\n",
    "        target_size=(150, 150),\n",
    "    )\n",
    "    return train_generator, val_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DIR = 'D:/animals classification/animals/training/'\n",
    "VALIDATION_DIR = 'D:/animals classification/animals/validation/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9191 images belonging to 4 classes.\n",
      "Found 2300 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator, validation_generator = train_val_generators(TRAINING_DIR, VALIDATION_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Conv2D(\n",
    "                32,\n",
    "                kernel_size=(3, 3),\n",
    "                strides=1,\n",
    "                padding=\"same\",\n",
    "                activation=\"relu\",\n",
    "                input_shape=(150, 150, 3),\n",
    "            ),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding=\"same\"),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Conv2D(\n",
    "                64,\n",
    "                kernel_size=(3, 3),\n",
    "                strides=1,\n",
    "                padding=\"same\",\n",
    "                activation=\"relu\",\n",
    "            ),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding=\"same\"),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Conv2D(\n",
    "                128,\n",
    "                kernel_size=(3, 3),\n",
    "                strides=1,\n",
    "                padding=\"same\",\n",
    "                activation=\"relu\",\n",
    "            ),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding=\"same\"),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Conv2D(\n",
    "                256,\n",
    "                kernel_size=(3, 3),\n",
    "                strides=1,\n",
    "                padding=\"same\",\n",
    "                activation=\"relu\",\n",
    "            ),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding=\"same\"),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(0.4),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dense(1028, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(4, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg19 (Functional)          (None, 512)               20024384  \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 4)                 2052      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,026,436\n",
      "Trainable params: 2,052\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.applications import VGG19\n",
    "vgg_model = Sequential()\n",
    "vgg_model.add(VGG19(include_top=False, pooling='avg', weights='imagenet'))\n",
    "# vgg_model.add(tf.keras.layers.Conv2D(64, kernel_size=(3,3), strides=1, padding='same'))\n",
    "# vgg_model.add(tf.keras.layers.BatchNormalization())\n",
    "# vgg_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "# vgg_model.add(tf.keras.layers.Dropout(0.3))\n",
    "# vgg_model.add(tf.keras.layers.Dense(1028, activation='relu'))\n",
    "vgg_model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "vgg_model.layers[0].trainable = False\n",
    "vgg_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "144/144 [==============================] - 27s 181ms/step - loss: 1.1778 - accuracy: 0.4759 - val_loss: 1.0572 - val_accuracy: 0.5761\n",
      "Epoch 2/30\n",
      "144/144 [==============================] - 26s 177ms/step - loss: 0.9791 - accuracy: 0.6252 - val_loss: 0.9404 - val_accuracy: 0.6535\n",
      "Epoch 3/30\n",
      "144/144 [==============================] - 26s 177ms/step - loss: 0.8909 - accuracy: 0.6701 - val_loss: 0.8741 - val_accuracy: 0.6613\n",
      "Epoch 4/30\n",
      "144/144 [==============================] - 26s 178ms/step - loss: 0.8353 - accuracy: 0.6902 - val_loss: 0.8401 - val_accuracy: 0.6891\n",
      "Epoch 5/30\n",
      "144/144 [==============================] - 26s 178ms/step - loss: 0.8058 - accuracy: 0.6993 - val_loss: 0.8118 - val_accuracy: 0.6913\n",
      "Epoch 6/30\n",
      "144/144 [==============================] - 25s 176ms/step - loss: 0.7744 - accuracy: 0.7092 - val_loss: 0.7702 - val_accuracy: 0.7013\n",
      "Epoch 7/30\n",
      "144/144 [==============================] - 26s 177ms/step - loss: 0.7585 - accuracy: 0.7143 - val_loss: 0.7699 - val_accuracy: 0.7135\n",
      "Epoch 8/30\n",
      "144/144 [==============================] - 25s 176ms/step - loss: 0.7437 - accuracy: 0.7189 - val_loss: 0.7483 - val_accuracy: 0.7109\n",
      "Epoch 9/30\n",
      "144/144 [==============================] - 26s 177ms/step - loss: 0.7295 - accuracy: 0.7284 - val_loss: 0.7326 - val_accuracy: 0.7265\n",
      "Epoch 10/30\n",
      "144/144 [==============================] - 25s 176ms/step - loss: 0.7166 - accuracy: 0.7369 - val_loss: 0.7301 - val_accuracy: 0.7252\n",
      "Epoch 11/30\n",
      "144/144 [==============================] - 26s 177ms/step - loss: 0.6997 - accuracy: 0.7409 - val_loss: 0.7058 - val_accuracy: 0.7422\n",
      "Epoch 12/30\n",
      "144/144 [==============================] - 25s 176ms/step - loss: 0.6928 - accuracy: 0.7440 - val_loss: 0.7054 - val_accuracy: 0.7391\n",
      "Epoch 13/30\n",
      "144/144 [==============================] - 26s 177ms/step - loss: 0.6833 - accuracy: 0.7473 - val_loss: 0.6920 - val_accuracy: 0.7500\n",
      "Epoch 14/30\n",
      "144/144 [==============================] - 26s 179ms/step - loss: 0.6828 - accuracy: 0.7418 - val_loss: 0.6938 - val_accuracy: 0.7574\n",
      "Epoch 15/30\n",
      "144/144 [==============================] - 26s 179ms/step - loss: 0.6724 - accuracy: 0.7488 - val_loss: 0.6799 - val_accuracy: 0.7522\n",
      "Epoch 16/30\n",
      "144/144 [==============================] - 26s 177ms/step - loss: 0.6580 - accuracy: 0.7577 - val_loss: 0.6771 - val_accuracy: 0.7539\n",
      "Epoch 17/30\n",
      "144/144 [==============================] - 25s 177ms/step - loss: 0.6665 - accuracy: 0.7484 - val_loss: 0.6935 - val_accuracy: 0.7378\n",
      "Epoch 18/30\n",
      "144/144 [==============================] - 25s 176ms/step - loss: 0.6616 - accuracy: 0.7511 - val_loss: 0.6896 - val_accuracy: 0.7439\n",
      "Epoch 19/30\n",
      "144/144 [==============================] - 26s 177ms/step - loss: 0.6533 - accuracy: 0.7577 - val_loss: 0.6810 - val_accuracy: 0.7504\n",
      "Epoch 20/30\n",
      "144/144 [==============================] - 25s 177ms/step - loss: 0.6576 - accuracy: 0.7545 - val_loss: 0.6804 - val_accuracy: 0.7465\n",
      "Epoch 21/30\n",
      "144/144 [==============================] - 25s 176ms/step - loss: 0.6412 - accuracy: 0.7591 - val_loss: 0.6672 - val_accuracy: 0.7539\n",
      "Epoch 22/30\n",
      "144/144 [==============================] - 26s 177ms/step - loss: 0.6409 - accuracy: 0.7603 - val_loss: 0.6593 - val_accuracy: 0.7548\n",
      "Epoch 23/30\n",
      "144/144 [==============================] - 25s 177ms/step - loss: 0.6409 - accuracy: 0.7598 - val_loss: 0.6521 - val_accuracy: 0.7565\n",
      "Epoch 24/30\n",
      "144/144 [==============================] - 26s 177ms/step - loss: 0.6415 - accuracy: 0.7572 - val_loss: 0.6531 - val_accuracy: 0.7557\n",
      "Epoch 25/30\n",
      "144/144 [==============================] - 26s 177ms/step - loss: 0.6292 - accuracy: 0.7647 - val_loss: 0.6533 - val_accuracy: 0.7513\n",
      "Epoch 26/30\n",
      "144/144 [==============================] - 25s 176ms/step - loss: 0.6240 - accuracy: 0.7674 - val_loss: 0.6547 - val_accuracy: 0.7583\n",
      "Epoch 27/30\n",
      "144/144 [==============================] - 26s 177ms/step - loss: 0.6235 - accuracy: 0.7688 - val_loss: 0.6600 - val_accuracy: 0.7626\n",
      "Epoch 28/30\n",
      "144/144 [==============================] - 26s 178ms/step - loss: 0.6314 - accuracy: 0.7624 - val_loss: 0.6391 - val_accuracy: 0.7613\n",
      "Epoch 29/30\n",
      "144/144 [==============================] - 26s 178ms/step - loss: 0.6260 - accuracy: 0.7686 - val_loss: 0.6585 - val_accuracy: 0.7565\n",
      "Epoch 30/30\n",
      "144/144 [==============================] - 25s 177ms/step - loss: 0.6220 - accuracy: 0.7665 - val_loss: 0.6480 - val_accuracy: 0.7604\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f8007cf7f0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_model.fit(train_generator,\n",
    "              epochs=30,\n",
    "              verbose=1,\n",
    "              validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 150, 150, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 75, 75, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 75, 75, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 75, 75, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 38, 38, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 38, 38, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 38, 38, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 19, 19, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 19, 19, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 19, 19, 256)       295168    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 10, 10, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 10, 10, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 10, 10, 256)       0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25600)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               13107712  \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1028)              527364    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4)                 4116      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,031,576\n",
      "Trainable params: 14,029,592\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_generator,\n",
    "                    epochs=30,\n",
    "                    verbose=1,\n",
    "                    validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'r', \"Training Accuracy\")\n",
    "plt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.show()\n",
    "print(\"\")\n",
    "\n",
    "plt.plot(epochs, loss, 'r', \"Training Loss\")\n",
    "plt.plot(epochs, val_loss, 'b', \"Validation Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "animal-classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
